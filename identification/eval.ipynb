{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from network import MMPeptide, SEQPeptide, VoxPeptide, MMFPeptide\n",
    "from swinunetr import SwinUNETR\n",
    "from ThermoGNN.model import GraphGNN\n",
    "from dataset import pdb_parser, AMAs\n",
    "from dataset_graph import PairData\n",
    "from torchmetrics import F1Score, Accuracy, AveragePrecision, AUROC\n",
    "import torch\n",
    "import networkx as nx\n",
    "from Bio.PDB.Polypeptide import three_to_one\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.loader import DataLoader as GDataLoader\n",
    "import numpy as np\n",
    "from utils import set_seed\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "basedir = os.path.abspath('../checkpoints')\n",
    "wdirs = [\n",
    "    'anti-gat-mlce1280.00252-10-50',\n",
    "    'anti-gcn-mlce1280.00252-10-50',\n",
    "    'anti-gin-mlce1280.00252-10-50',\n",
    "    'anti-graphsage-mlce1280.00252-10-50',\n",
    "    'anti-mm-ce1280.00250',\n",
    "    'anti-mm-mlce1280.00250',\n",
    "    'anti-seq-ce1280.00250',\n",
    "    'anti-voxel-ce1280.00250',\n",
    "    'anti-voxel-tr-ce160.00230',\n",
    "]\n",
    "simi = 64\n",
    "\n",
    "def main(weight_dir, perform):\n",
    "    if not os.path.exists(weight_dir):\n",
    "        raise ValueError\n",
    "    args = load_args(weight_dir)\n",
    "    \n",
    "    set_seed(args.seed)\n",
    "\n",
    "    logging.basicConfig(handlers=[\n",
    "        logging.StreamHandler()],\n",
    "        format=\"%(asctime)s: %(message)s\", datefmt=\"%F %T\", level=logging.INFO)\n",
    "\n",
    "    logging.info(f'saving_dir: {weight_dir}')\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    voxel_flag = True\n",
    "    if args.model == 'seq':\n",
    "        model = SEQPeptide(classes=args.classes, q_encoder='mlp')\n",
    "    elif args.model == 'voxel':\n",
    "        model = VoxPeptide(classes=args.classes)\n",
    "    elif args.model == 'mm':\n",
    "        model = MMPeptide(classes=args.classes, q_encoder='mlp')\n",
    "    elif args.model == 'mmf':\n",
    "        model = MMFPeptide(classes=args.classes, q_encoder='mlp')\n",
    "    elif args.model == 'voxel-tr':\n",
    "        model = SwinUNETR(img_size=(64, 64, 64), in_channels=3, classes=args.classes)\n",
    "    else:\n",
    "        model = GraphGNN(num_layer=args.num_layer, input_dim=20, emb_dim=args.emb_dim, out_dim=args.classes, JK=\"last\",\n",
    "                         drop_ratio=args.dropout_ratio, graph_pooling=args.graph_pooling, gnn_type=args.gnn_type)\n",
    "        voxel_flag = False\n",
    "    model.to(device)\n",
    "        \n",
    "    logging.info('Loading Test Dataset')\n",
    "    qlx_set = advs24(voxel_flag=voxel_flag, max_length=50)\n",
    "    if voxel_flag:\n",
    "        qlx_loader = DataLoader(qlx_set, batch_size=1, shuffle=False)\n",
    "    else:\n",
    "        qlx_loader = GDataLoader(qlx_set, batch_size=1, follow_batch=['x_s'], shuffle=False)\n",
    "\n",
    "    logging.info(f'Test set:advs2024 {len(qlx_set)}')\n",
    "    \n",
    "    class_performs = [[] for _ in range(5)]\n",
    "    for fold in range(5):\n",
    "        weights_path = f\"{weight_dir}/model_{fold + 1}.pth\"\n",
    "        model.load_state_dict(torch.load(weights_path))\n",
    "        logging.info(f'Running Cross Validation {fold + 1}')\n",
    "\n",
    "        class_performs[fold] = eval(args, model, qlx_loader, device, voxel_flag)\n",
    "\n",
    "    logging.info(f'Cross Validation Finished!')\n",
    "\n",
    "    qlx_perform_list = np.asarray(class_performs).reshape(5, 24)\n",
    "    taskname = os.path.basename(weight_dir)\n",
    "    perform.write(','.join(sum([[taskname] + [str(x) for x in np.mean(qlx_perform_list, 0)[i:i+4]] for i in range(0, 24, 4)], []))+'\\n')\n",
    "    perform.write(','.join(sum([['std'] + [str(x) for x in np.std(qlx_perform_list, 0)[i:i+4]] for i in range(0, 24, 4)], []))+'\\n')\n",
    "\n",
    "def eval(args, model, valid_loader, device, voxel_flag):\n",
    "    num_labels = 4\n",
    "    avg = 'none'\n",
    "    if num_labels == 1:\n",
    "        task = 'binary'\n",
    "    else:\n",
    "        task = 'multilabel'\n",
    "    metric_acc = Accuracy(average=avg, task=task, num_labels=num_labels).to(device)\n",
    "    metric_f1 = F1Score(average=avg, task=task, num_labels=num_labels).to(device)\n",
    "    metric_ap = AveragePrecision(average=avg, task=task, num_labels=num_labels).to(device)\n",
    "    metric_auc = AUROC(average=avg, task=task, num_labels=num_labels).to(device)\n",
    "    \n",
    "    avg = 'micro'\n",
    "    micro_acc = Accuracy(average=avg, task=task, num_labels=num_labels).to(device)\n",
    "    micro_f1 = F1Score(average=avg, task=task, num_labels=num_labels).to(device)\n",
    "    micro_ap = AveragePrecision(average=avg, task=task, num_labels=num_labels).to(device)\n",
    "    micro_auc = AUROC(average=avg, task=task, num_labels=num_labels).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            voxel, seq, gt = data\n",
    "            gts.append(gt.to(device))\n",
    "            if voxel_flag:\n",
    "                out = model((voxel.to(device), seq.to(device)))\n",
    "            else:\n",
    "                out = model(voxel.to(device))\n",
    "            preds.append(out)\n",
    "\n",
    "    preds = torch.nn.functional.sigmoid(torch.cat(preds, dim=0))[:, [1, 2, 4, 5]]\n",
    "    gts = torch.cat(gts, dim=0).int()[:, [1, 2, 4, 5]]\n",
    "\n",
    "    ap = metric_ap(preds, gts).cpu().detach().numpy()\n",
    "    f1 = metric_f1(preds, gts).cpu().detach().numpy()\n",
    "    acc = metric_acc(preds, gts).cpu().detach().numpy()\n",
    "    auc = metric_auc(preds, gts).cpu().detach().numpy()\n",
    "    macros = np.expand_dims(np.mean(np.stack((ap, f1, acc, auc), axis=0), axis=-1), axis=0)\n",
    "    micros = np.asarray([[micro_ap(preds, gts).item(), micro_f1(preds, gts).item(), \n",
    "                         micro_acc(preds, gts).item(), micro_auc(preds, gts).item()]])\n",
    "    results = np.concatenate((macros, micros, np.stack((ap, f1, acc, auc), axis=-1)), axis=0)\n",
    "    return results\n",
    "\n",
    "class advs24(Dataset):\n",
    "    def __init__(self, voxel_flag, max_length=50):\n",
    "        self.num_classes = 6\n",
    "        self.max_length=50\n",
    "        self.voxel_flag = voxel_flag\n",
    "        self.p = PDBParser(QUIET=True)\n",
    "        if voxel_flag:\n",
    "            processer = self.voxelprocess\n",
    "        else:\n",
    "            processer = self.graphprocess\n",
    "\n",
    "        all_data = pd.read_csv('advs2024b.csv', encoding=\"unicode_escape\").values\n",
    "        filtered = pd.read_csv(f'./simi/{simi}.csv')['FileB_Sequence'].str.upper().str.strip().unique().tolist()\n",
    "\n",
    "        idx_list, seq_list, labels = all_data[:, 0], all_data[:, 1], all_data[:, [2, 3, 4, 5, 6, 8]].astype(np.float32)\n",
    "\n",
    "        filter_idx_list = []\n",
    "        seq_new_list = []\n",
    "        label_list = []\n",
    "        for idx in range(len(idx_list)):\n",
    "            seq = seq_list[idx].upper().strip()\n",
    "            if seq in filtered:\n",
    "                continue\n",
    "            if 'X' in seq or 'U' in seq or 'O' in seq or len(seq) > max_length or len(seq) < 6:\n",
    "                continue\n",
    "\n",
    "            filter_idx_list.append(idx)\n",
    "            seq_new_list.append(seq)\n",
    "            label_list.append(labels[idx])\n",
    "\n",
    "        self.data_list = []\n",
    "        for i in tqdm(range(len(filter_idx_list))):\n",
    "            idx = filter_idx_list[i]\n",
    "            seq = seq_new_list[i]\n",
    "            label = label_list[i]\n",
    "            if os.path.exists('./advs2024/' + seq + \".pdb\"):\n",
    "                pdb_path = './advs2024/' + seq + \".pdb\"\n",
    "            else:\n",
    "                print(f'lacking pdb file {seq}')\n",
    "                continue\n",
    "            \n",
    "            processer(idx, pdb_path, seq, label)\n",
    "                \n",
    "    def voxelprocess(self, idx, pdb_path, seq, label):\n",
    "        voxel, _ = pdb_parser(self.p, idx, pdb_path)\n",
    "        seq_emb = [AMAs[char] for char in seq] + [0] * (self.max_length - len(seq))\n",
    "        self.data_list.append((voxel, seq_emb, label))\n",
    "        return voxel, seq_emb, label\n",
    "    \n",
    "    def graphprocess(self, idx, pdb_path, seq, label):\n",
    "        structure = self.p.get_structure(idx, pdb_path)\n",
    "        G = nx.Graph()\n",
    "        flag = False\n",
    "        for i in structure[0]:\n",
    "            if i.id != 'A':\n",
    "                flag = True\n",
    "        if flag:\n",
    "            return\n",
    "\n",
    "        chain = structure[0]['A']\n",
    "        for res in chain:\n",
    "            if is_aa(res.get_resname(), standard=True):\n",
    "                resname = res.get_resname()\n",
    "                G.add_node(res.id[1], name=resname)\n",
    "\n",
    "        num_nodes = len(G.nodes)\n",
    "        nodes_list = list(G.nodes)\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 1, num_nodes):\n",
    "                m = nodes_list[i]\n",
    "                n = nodes_list[j]\n",
    "                distance = chain[m][\"CA\"] - chain[n][\"CA\"]\n",
    "\n",
    "                if distance <= 5:\n",
    "                    G.add_edge(m, n, weight=5 / distance)\n",    
    "        G = nx.convert_node_labels_to_integers(G)\n",
    "\n",
    "        def load_aa_features(feature_path):\n",
    "            aa_features = {}\n",
    "            for line in open(feature_path):\n",
    "                line = line.strip().split()\n",
    "                aa, features = line[0], line[1:]\n",
    "                features = [float(feature) for feature in features]\n",
    "                aa_features[aa] = features\n",
    "            return aa_features\n",
    "\n",
    "        aa_features = load_aa_features('../features.txt')\n",
    "        features = []\n",
    "        for node in nodes_list:\n",
    "            res = chain[int(node)]\n",
    "            aa_feature = aa_features[three_to_one(res.get_resname())]\n",
    "            features.append(aa_feature)\n",
    "        for i, node in enumerate(G.nodes.data()):\n",
    "            node[1]['x'] = features[i]\n",
    "        data_wt = from_networkx(G)\n",
    "\n",
    "        data_graph = PairData(data_wt.edge_index, data_wt.x)\n",
    "        seq_emb = [AMAs[char] for char in seq] + [0] * (self.max_length - len(seq))\n",
    "        self.data_list.append((data_graph, seq_emb, label))\n",
    "        return data_graph, seq_emb, label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        voxel, seq_emb, gt = self.data_list[idx]\n",
    "        if self.voxel_flag:\n",
    "            return torch.Tensor(voxel).float(), torch.Tensor(seq_emb), torch.Tensor(gt)\n",
    "        else:\n",
    "            return voxel, torch.Tensor(seq_emb), torch.Tensor(gt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "def load_args(weight_dir):\n",
    "    with open(os.path.join(weight_dir, \"config.json\"), \"r\") as f:\n",
    "        args_dict = json.load(f)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    for key in args_dict:\n",
    "        parser.add_argument(f'--{key}')\n",
    "    args = parser.parse_args([])\n",
    "    args.__dict__.update(args_dict)\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(64, 81, 2):\n",
    "        simi = i\n",
    "        perform = open(f'./results/{simi}.csv', 'w')\n",
    "        perform.write('Macro Average,AP,F1,ACC,AUC,Micro Average,AP,F1,ACC,AUC,Pseudomonas aeruginosa,AP,F1,ACC,AUC,Staphylococcus aureus,AP,F1,ACC,AUC,Enterobacteriaceae,AP,F1,ACC,AUC,Salmonella species,AP,F1,ACC,AUC\\n')\n",
    "        for dir in wdirs:\n",
    "            wdir = os.path.join(basedir, dir)\n",
    "            main(wdir, perform)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
